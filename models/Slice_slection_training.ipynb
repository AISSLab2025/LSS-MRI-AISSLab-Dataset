{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2863e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os, json, random, traceback\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# --------------------------- SEED ---------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---------------------- GPU SPEED SETTINGS ------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# ------------------------- PARAMS ---------------------------\n",
    "NUM_SLICES = 18\n",
    "IMG_SIZE   = 256\n",
    "\n",
    "LEVELS = [\"L1-L2\",\"L2-L3\",\"L3-L4\",\"L4-L5\",\"L5-S1\"]\n",
    "SIDES  = [\"left\",\"right\"]\n",
    "TARGET_KEYS = [f\"{l}_{s}\" for l in LEVELS for s in SIDES]  # 10 outputs\n",
    "\n",
    "# ------------------------- PATHS ----------------------------\n",
    "DICOM_ROOT = r\"D:\\Submitted Matrial (conference&journal)\\Sagittal Data Artical\\V0.47 Dataset analysis\\dataspitting\\split_dataset\\images\\train_val\"\n",
    "LABEL_ROOT = r\"D:\\Submitted Matrial (conference&journal)\\Sagittal Data Artical\\V0.47 Dataset analysis\\dataspitting\\split_dataset\\label\\train_val\"\n",
    "CACHE_ROOT = r\"D:\\Submitted Matrial (conference&journal)\\Sagittal Data Artical\\V0.47 Dataset analysis\\dataspitting\\split_dataset\\cache_npy\"\n",
    "\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------------------- TRAINING CONFIG ---------------------\n",
    "EPOCHS     = 100\n",
    "LR         = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# IMPORTANT: start with 0 to DEBUG, then set 4 or 8 after stable\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "BEST_MODEL_PATH = \"best_model_convnext3d_regression.pth\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                   PREPROCESS + CACHE\n",
    "# ============================================================\n",
    "\n",
    "def extract_foreground(img, threshold=10):\n",
    "    mask = img > threshold\n",
    "    if not np.any(mask):\n",
    "        return img\n",
    "    coords = np.column_stack(np.where(mask))\n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    return img[y_min:y_max+1, x_min:x_max+1]\n",
    "\n",
    "def extract_labels(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    label = []\n",
    "    mask  = []\n",
    "    for key in TARGET_KEYS:\n",
    "        lvl, side = key.split(\"_\")\n",
    "        coord = data.get(lvl, {}).get(side)\n",
    "        if coord is None:\n",
    "            label.append(0.0)\n",
    "            mask.append(0.0)\n",
    "        else:\n",
    "            z = coord[2]\n",
    "            z_index = int(round(z * NUM_SLICES))\n",
    "            z_index = max(0, min(NUM_SLICES - 1, z_index))\n",
    "            label.append(float(z_index))\n",
    "            mask.append(1.0)\n",
    "\n",
    "    return np.array(label, dtype=np.float32), np.array(mask, dtype=np.float32)\n",
    "\n",
    "def load_and_preprocess_volume(dicom_dir):\n",
    "    files = [os.path.join(dicom_dir, f) for f in os.listdir(dicom_dir) if f.endswith(\".dcm\")]\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(f\"No DICOM files in {dicom_dir}\")\n",
    "\n",
    "    def sort_key(p):\n",
    "        try:\n",
    "            return int(pydicom.dcmread(p, stop_before_pixels=True).InstanceNumber)\n",
    "        except Exception:\n",
    "            return p\n",
    "\n",
    "    files = sorted(files, key=sort_key)\n",
    "\n",
    "    # enforce NUM_SLICES\n",
    "    if len(files) > NUM_SLICES:\n",
    "        s = (len(files) - NUM_SLICES) // 2\n",
    "        files = files[s:s + NUM_SLICES]\n",
    "    elif len(files) < NUM_SLICES:\n",
    "        files = files + [files[-1]] * (NUM_SLICES - len(files))\n",
    "\n",
    "    vol = []\n",
    "    for p in files:\n",
    "        ds = pydicom.dcmread(p)\n",
    "        img = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "        img = extract_foreground(img)\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "        vol.append(img)\n",
    "\n",
    "    vol = np.stack(vol).astype(np.float32)  # (D,H,W)\n",
    "    if vol.shape != (NUM_SLICES, IMG_SIZE, IMG_SIZE):\n",
    "        raise RuntimeError(f\"Bad volume shape {vol.shape} in {dicom_dir}\")\n",
    "\n",
    "    return vol\n",
    "\n",
    "def cache_patient(pid):\n",
    "    out_dir = os.path.join(CACHE_ROOT, pid)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    vol_out  = os.path.join(out_dir, \"volume.npy\")\n",
    "    lab_out  = os.path.join(out_dir, \"label.npy\")\n",
    "    mask_out = os.path.join(out_dir, \"mask.npy\")\n",
    "\n",
    "    # already cached\n",
    "    if os.path.exists(vol_out) and os.path.exists(lab_out) and os.path.exists(mask_out):\n",
    "        return True\n",
    "\n",
    "    dicom_dir = os.path.join(DICOM_ROOT, pid)\n",
    "    json_path = os.path.join(LABEL_ROOT, f\"{pid}.json\")\n",
    "\n",
    "    if not os.path.isdir(dicom_dir) or not os.path.exists(json_path):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        vol = load_and_preprocess_volume(dicom_dir)\n",
    "        label, mask = extract_labels(json_path)\n",
    "        np.save(vol_out, vol)\n",
    "        np.save(lab_out, label)\n",
    "        np.save(mask_out, mask)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Cache failed for {pid}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_cache(pid):\n",
    "    \"\"\"Return True if sample is readable and valid.\"\"\"\n",
    "    base = os.path.join(CACHE_ROOT, pid)\n",
    "    try:\n",
    "        vol = np.load(os.path.join(base, \"volume.npy\"))\n",
    "        y   = np.load(os.path.join(base, \"label.npy\"))\n",
    "        m   = np.load(os.path.join(base, \"mask.npy\"))\n",
    "\n",
    "        if vol.shape != (NUM_SLICES, IMG_SIZE, IMG_SIZE):\n",
    "            return False\n",
    "        if y.shape != (10,) or m.shape != (10,):\n",
    "            return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                   DATASET (SAFE)\n",
    "# ============================================================\n",
    "\n",
    "class CachedNPYDataset(Dataset):\n",
    "    def __init__(self, ids):\n",
    "        self.ids = ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.ids[idx]\n",
    "        base = os.path.join(CACHE_ROOT, pid)\n",
    "\n",
    "        # IMPORTANT: if something is wrong, raise a clear error\n",
    "        vol  = np.load(os.path.join(base, \"volume.npy\"))\n",
    "        y    = np.load(os.path.join(base, \"label.npy\"))\n",
    "        m    = np.load(os.path.join(base, \"mask.npy\"))\n",
    "\n",
    "        vol = torch.from_numpy(vol).unsqueeze(0)  # (1,D,H,W)\n",
    "        y   = torch.from_numpy(y)\n",
    "        m   = torch.from_numpy(m)\n",
    "\n",
    "        return vol, y, m\n",
    "\n",
    "\n",
    "def safe_collate(batch):\n",
    "    \"\"\"Drop None/bad samples (extra safety).\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.default_collate(batch)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                    MODEL (FAST + STABLE)\n",
    "# ============================================================\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.dw  = nn.Conv3d(c, c, kernel_size=(3,7,7), padding=(1,3,3), groups=c, bias=False)\n",
    "        self.gn  = nn.GroupNorm(1, c)\n",
    "        self.pw1 = nn.Conv3d(c, 4*c, kernel_size=1, bias=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.pw2 = nn.Conv3d(4*c, c, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pw2(self.act(self.pw1(self.gn(self.dw(x)))))\n",
    "\n",
    "class ConvNext3D(nn.Module):\n",
    "    def __init__(self, n_outputs=10):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv3d(1, 64, kernel_size=(1,4,4), stride=(1,4,4), bias=False)\n",
    "        self.b1   = Block(64)\n",
    "\n",
    "        self.d1   = nn.Conv3d(64, 128, kernel_size=2, stride=2, bias=False)\n",
    "        self.b2   = Block(128)\n",
    "\n",
    "        self.d2   = nn.Conv3d(128, 256, kernel_size=2, stride=2, bias=False)\n",
    "        self.b3   = Block(256)\n",
    "\n",
    "        self.d3   = nn.Conv3d(256, 512, kernel_size=2, stride=2, bias=False)\n",
    "        self.b4   = Block(512)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc   = nn.Linear(512, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.b1(self.stem(x))\n",
    "        x = self.b2(self.d1(x))\n",
    "        x = self.b3(self.d2(x))\n",
    "        x = self.b4(self.d3(x))\n",
    "        x = self.pool(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                     LOSS + EVAL + TRAIN\n",
    "# ============================================================\n",
    "\n",
    "def masked_mse_loss(pred, target, mask, eps=1e-6):\n",
    "    return (((pred - target) ** 2) * mask).sum() / (mask.sum() + eps)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    for batch in loader:\n",
    "        if batch is None:\n",
    "            continue\n",
    "        vol, y, m = batch\n",
    "        vol = vol.to(device, non_blocking=True).to(memory_format=torch.channels_last_3d)\n",
    "        y   = y.to(device, non_blocking=True)\n",
    "        m   = m.to(device, non_blocking=True)\n",
    "        with autocast(enabled=(device.type == \"cuda\")):\n",
    "            pred = model(vol)\n",
    "            loss = masked_mse_loss(pred, y, m)\n",
    "        total += float(loss.item())\n",
    "        n += 1\n",
    "    return total / max(n, 1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ------------------ CACHE STAGE ------------------\n",
    "    all_pids = [p for p in os.listdir(DICOM_ROOT) if os.path.isdir(os.path.join(DICOM_ROOT, p))]\n",
    "    print(f\"Found {len(all_pids)} patient folders.\")\n",
    "\n",
    "    cached = []\n",
    "    for pid in tqdm(all_pids, desc=\"Caching\"):\n",
    "        ok = cache_patient(pid)\n",
    "        if ok:\n",
    "            cached.append(pid)\n",
    "\n",
    "    print(f\"Cached candidates: {len(cached)}\")\n",
    "\n",
    "    # ------------------ VERIFY CACHE (CRITICAL) ------------------\n",
    "    good = []\n",
    "    bad = []\n",
    "    for pid in tqdm(cached, desc=\"Verifying cache\"):\n",
    "        if verify_cache(pid):\n",
    "            good.append(pid)\n",
    "        else:\n",
    "            bad.append(pid)\n",
    "\n",
    "    print(f\"✅ Good cached samples: {len(good)}\")\n",
    "    print(f\"❌ Bad cached samples:  {len(bad)}\")\n",
    "\n",
    "    # Save bad list\n",
    "    if len(bad) > 0:\n",
    "        with open(\"bad_samples.txt\", \"w\") as f:\n",
    "            for pid in bad:\n",
    "                f.write(pid + \"\\n\")\n",
    "        print(\"Saved bad sample IDs to bad_samples.txt\")\n",
    "\n",
    "    if len(good) < 5:\n",
    "        raise RuntimeError(\"Too few valid samples. Check cache/paths.\")\n",
    "\n",
    "    # ------------------ SPLIT ------------------\n",
    "    train_ids, val_ids = train_test_split(good, test_size=0.125, random_state=SEED)\n",
    "\n",
    "    train_ds = CachedNPYDataset(train_ids)\n",
    "    val_ds   = CachedNPYDataset(val_ids)\n",
    "\n",
    "    # IMPORTANT: start with NUM_WORKERS=0 to see true error, then increase later\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(NUM_WORKERS > 0),\n",
    "        collate_fn=safe_collate\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(NUM_WORKERS > 0),\n",
    "        collate_fn=safe_collate\n",
    "    )\n",
    "\n",
    "    # ------------------ MODEL ------------------\n",
    "    model = ConvNext3D(n_outputs=10).to(device).to(memory_format=torch.channels_last_3d)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # ------------------ TRAIN ------------------\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss, batches = 0.0, 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        for batch in pbar:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            vol, y, m = batch\n",
    "\n",
    "            vol = vol.to(device, non_blocking=True).to(memory_format=torch.channels_last_3d)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "            m   = m.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=(device.type == \"cuda\")):\n",
    "                pred = model(vol)\n",
    "                loss = masked_mse_loss(pred, y, m)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += float(loss.item())\n",
    "            batches += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train = epoch_loss / max(batches, 1)\n",
    "        val_loss  = evaluate(model, val_loader)\n",
    "\n",
    "        train_losses.append(avg_train)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train {avg_train:.4f} | Val {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(f\"✅ Saved Best Model: {BEST_MODEL_PATH} (Val {best_val:.4f})\")\n",
    "\n",
    "    # ------------------ PLOT ------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, EPOCHS+1), train_losses, label=\"Train Loss\")\n",
    "    plt.plot(range(1, EPOCHS+1), val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Train/Val Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"✅ Done.\")\n",
    "    print(\"\\nNEXT STEP:\")\n",
    "    print(\"Set NUM_WORKERS=4 or 8 after it runs without errors for speed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# ===================== PATHS =====================\n",
    "images_root = r\"D:\\Submitted Matrial (conference&journal)\\Sagittal Data Artical\\V0.47 Dataset analysis\\dataspitting\\split_dataset\\images\"\n",
    "dicom_root  = os.path.join(images_root, \"test\")   # <-- IMPORTANT\n",
    "label_root  = r\"D:\\Submitted Matrial (conference&journal)\\Sagittal Data Artical\\V0.47 Dataset analysis\\dataspitting\\split_dataset\\label\\test\"\n",
    "\n",
    "# ===================== DATASET =====================\n",
    "class LumbarDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # list of tuples: (volume, (label, mask)) OR (volume, label) depending on your extract_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vol, labels = self.data[idx]\n",
    "\n",
    "        # vol: (D,H,W) -> torch (1,D,H,W)\n",
    "        vol = torch.from_numpy(vol).unsqueeze(0).float()\n",
    "\n",
    "        # If extract_labels returns (label, mask)\n",
    "        if isinstance(labels, tuple) and len(labels) == 2:\n",
    "            y, m = labels\n",
    "            y = torch.from_numpy(y).float()\n",
    "            m = torch.from_numpy(m).float()\n",
    "            return vol, y, m\n",
    "\n",
    "        # If extract_labels returns only label\n",
    "        y = torch.from_numpy(labels).float()\n",
    "        return vol, y\n",
    "\n",
    "# ===================== BUILD TEST DATA =====================\n",
    "test_data = []\n",
    "for patient_id in os.listdir(dicom_root):\n",
    "    vol_path   = os.path.join(dicom_root, patient_id)          # folder that contains .dcm slices\n",
    "    label_path = os.path.join(label_root, f\"{patient_id}.json\")\n",
    "\n",
    "    if not os.path.isdir(vol_path):\n",
    "        continue\n",
    "    if not os.path.exists(label_path):\n",
    "        continue\n",
    "\n",
    "    volume = load_and_preprocess_volume(vol_path)\n",
    "\n",
    "    # IMPORTANT:\n",
    "    # - if your extract_labels returns (label, mask) keep as is\n",
    "    # - if it returns only label, keep as is\n",
    "    labels = extract_labels(label_path)\n",
    "\n",
    "    test_data.append((volume, labels))\n",
    "\n",
    "print(f\"Total test samples: {len(test_data)}\")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    LumbarDataset(test_data),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,   # Windows safe\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `---------------------- MODEL ----------------------\n",
    "model = ConvNext3D(n_outputs=10)   # create model first\n",
    "model = model.to(device).to(memory_format=torch.channels_last_3d)\n",
    "\n",
    "# ---------------------- LOAD BEST MODEL ----------------------\n",
    "model_path = r\"D:\\Submitted Matrial (conference&journal)\\Sagittal Data Artical\\V0.47 Dataset analysis\\dataspitting\\best_model_convnext3d_regression.pth\"\n",
    "state = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "model.eval()  # IMPORTANT\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_masks = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for vol, target, mask in test_loader:\n",
    "        vol    = vol.to(device, non_blocking=True).to(memory_format=torch.channels_last_3d)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        mask   = mask.to(device, non_blocking=True)\n",
    "\n",
    "        pred = model(vol)\n",
    "\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_targets.append(target.cpu())\n",
    "        all_masks.append(mask.cpu())\n",
    "\n",
    "all_preds   = torch.cat(all_preds, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "all_masks   = torch.cat(all_masks, dim=0)\n",
    "\n",
    "# ---------------------- ±1 slice PR / R / F1 / Accuracy ----------------------\n",
    "pred_round = torch.round(all_preds)\n",
    "valid_mask = all_masks > 0.5\n",
    "within_tol = torch.abs(pred_round - all_targets) <= 1.0\n",
    "\n",
    "TP = ((within_tol) & valid_mask).sum().item()\n",
    "FN = ((~within_tol) & valid_mask).sum().item()\n",
    "\n",
    "FP = 0  # no \"negative prediction\" in your setup\n",
    "\n",
    "precision = TP / max(TP + FP, 1)\n",
    "recall    = TP / max(TP + FN, 1)\n",
    "f1_score  = 2 * precision * recall / max(precision + recall, 1e-8)\n",
    "accuracy  = TP / max(TP + FN, 1)\n",
    "\n",
    "print(f\"Precision (±1 slice): {precision * 100:.2f}%\")\n",
    "print(f\"Recall    (±1 slice): {recall * 100:.2f}%\")\n",
    "print(f\"F1-score  (±1 slice): {f1_score * 100:.2f}%\")\n",
    "print(f\"Accuracy  (±1 slice): {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
